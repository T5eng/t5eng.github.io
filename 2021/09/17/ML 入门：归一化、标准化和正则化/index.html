<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>ML 入门：归一化、标准化和正则化 | Yubinnzeng&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="因为经常混淆Normalization和Regularization两个名词，所以搬运了一篇文章。原文：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;29957294 0x01 归一化 Normalization归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。 常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 Min-Max 归一">
<meta property="og:type" content="article">
<meta property="og:title" content="ML 入门：归一化、标准化和正则化">
<meta property="og:url" content="http://example.com/2021/09/17/ML%20%E5%85%A5%E9%97%A8%EF%BC%9A%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/index.html">
<meta property="og:site_name" content="Yubinnzeng&#39;s Blog">
<meta property="og:description" content="因为经常混淆Normalization和Regularization两个名词，所以搬运了一篇文章。原文：https:&#x2F;&#x2F;zhuanlan.zhihu.com&#x2F;p&#x2F;29957294 0x01 归一化 Normalization归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。 常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 Min-Max 归一">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-x_%7Bmin%7D%7D%7Bx_%7Bmax%7D-x_%7Bmin%7D%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=7.50%C3%9710%5E%7B9%7D/L">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-%5Cmu+%7D%7B%5Csigma+%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cmu">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Csigma">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/12/E4K9De.jpg">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/12/E4KiEd.jpg">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=J(w,b)">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=w">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=b">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=J(w,b)">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5E%7B%5B1%5D%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5E%7B%5B2%5D%7D">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/12/E4KCHH.jpg">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cgamma">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cbeta+">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=J(w,b)=+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(f(x),y)+%5Clambda+R(f)">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%CE%BB%E2%89%A50">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5E%7B%5B3%5D%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7Bp%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7Bp%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7Bp%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7B0%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B0%7D+=+%5C%23(i)%5C+with+%5C+x_%7Bi%7D+%5Cneq+0">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7B1%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D+=+%5Csum_%7Bi+=+1%7D%5E%7Bd%7D%5Clvert+x_i%5Crvert">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7B2%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B2%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5E2%5CBigr)%5E%7B1/2%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7Bp%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7Bp%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5Ep%5CBigr)%5E%7B1/p%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=%5ClVert+w%5CrVert_p">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=L_%7Bp%7D">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/12/E4KFUA.jpg">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7Bm%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D">
<meta property="og:image" content="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7B2m%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C%5E%7B2%7D_%7B2%7D">
<meta property="og:image" content="https://s2.ax1x.com/2019/05/12/E4KpuD.jpg">
<meta property="article:published_time" content="2021-09-17T11:02:27.164Z">
<meta property="article:modified_time" content="2021-09-17T12:37:10.588Z">
<meta property="article:author" content="Yubinn Zeng">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-x_%7Bmin%7D%7D%7Bx_%7Bmax%7D-x_%7Bmin%7D%7D">
  
    <link rel="alternate" href="/atom.xml" title="Yubinnzeng's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Yubinnzeng&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-ML 入门：归一化、标准化和正则化" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/09/17/ML%20%E5%85%A5%E9%97%A8%EF%BC%9A%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/" class="article-date">
  <time class="dt-published" datetime="2021-09-17T11:02:27.164Z" itemprop="datePublished">2021-09-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      ML 入门：归一化、标准化和正则化
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>因为经常混淆Normalization和Regularization两个名词，所以搬运了一篇文章。原文：<a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/29957294">https://zhuanlan.zhihu.com/p/29957294</a></p>
<h2 id="0x01-归一化-Normalization"><a href="#0x01-归一化-Normalization" class="headerlink" title="0x01 归一化 Normalization"></a><strong>0x01 归一化 Normalization</strong></h2><p>归一化一般是将数据映射到指定的范围，用于去除不同维度数据的量纲以及量纲单位。</p>
<p>常见的映射范围有 [0, 1] 和 [-1, 1] ，最常见的归一化方法就是 <strong>Min-Max 归一化</strong>：</p>
<h2 id="Min-Max-归一化"><a href="#Min-Max-归一化" class="headerlink" title="Min-Max 归一化"></a><strong>Min-Max 归一化</strong></h2><p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-x_%7Bmin%7D%7D%7Bx_%7Bmax%7D-x_%7Bmin%7D%7D"><img src="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-x_%7Bmin%7D%7D%7Bx_%7Bmax%7D-x_%7Bmin%7D%7D" alt="x_{new}=\frac{x-x_{min}}{x_{max}-x_{min}}"></a></p>
<p>举个例子，我们判断一个人的身体状况是否健康，那么我们会采集人体的很多指标，比如说：身高、体重、红细胞数量、白细胞数量等。</p>
<p>一个人身高 180cm，体重 70kg，白细胞计数<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=7.50%C3%9710%5E%7B9%7D/L"><img src="http://www.zhihu.com/equation?tex=7.50%C3%9710%5E%7B9%7D/L" alt="7.50×10^{9}/L"></a>，etc.</p>
<p>衡量两个人的状况时，白细胞计数就会起到主导作用从而<strong>遮盖住其他的特征</strong>，归一化后就不会有这样的问题。</p>
<h2 id="0x02-标准化-Normalization"><a href="#0x02-标准化-Normalization" class="headerlink" title="0x02 标准化 Normalization"></a><strong>0x02 标准化 Normalization</strong></h2><p><strong>在这里我们需要强调一下英文翻译的问题，在 Udacity 字幕组中对此进行了探讨：</strong></p>
<blockquote>
<p>归一化和标准化的英文翻译是一致的，但是根据其用途（或公式）的不同去理解（或翻译）</p>
</blockquote>
<p>下面我们将探讨最常见的标准化方法： <strong>Z-Score 标准化</strong>。</p>
<h2 id="Z-Score-标准化"><a href="#Z-Score-标准化" class="headerlink" title="Z-Score 标准化"></a><strong>Z-Score 标准化</strong></h2><p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-%5Cmu+%7D%7B%5Csigma+%7D"><img src="http://www.zhihu.com/equation?tex=x_%7Bnew%7D=%5Cfrac%7Bx-%5Cmu+%7D%7B%5Csigma+%7D" alt="x_{new}=\frac{x-\mu }{\sigma }"></a></p>
<p>其中<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cmu"><img src="http://www.zhihu.com/equation?tex=%5Cmu" alt="\mu"></a>是样本数据的<strong>均值（mean）</strong>，<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Csigma"><img src="http://www.zhihu.com/equation?tex=%5Csigma" alt="\sigma"></a>是样本数据的<strong>标准差（std）</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://s2.ax1x.com/2019/05/12/E4K9De.jpg"><img src="https://s2.ax1x.com/2019/05/12/E4K9De.jpg" alt="img"></a></p>
<p>上图则是一个散点序列的标准化过程：原图-&gt;减去均值-&gt;除以标准差。</p>
<p>显而易见，变成了一个<strong>均值为 0 ，方差为 1 的分布</strong>，下图通过 Cost 函数让我们更好的理解标准化的作用。</p>
<p><a target="_blank" rel="noopener" href="https://s2.ax1x.com/2019/05/12/E4KiEd.jpg"><img src="https://s2.ax1x.com/2019/05/12/E4KiEd.jpg" alt="img"></a></p>
<p>机器学习的目标无非就是不断优化损失函数，使其值最小。在上图中，<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=J(w,b)"><img src="http://www.zhihu.com/equation?tex=J(w,b)" alt="J(w,b)"></a>就是我们要优化的目标函数</p>
<p>我们不难看出，<strong>标准化后可以更加容易地得出最优参数</strong><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=w"><img src="http://www.zhihu.com/equation?tex=w" alt="w"></a><strong>和</strong><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=b"><img src="http://www.zhihu.com/equation?tex=b" alt="b"></a><strong>以及计算出</strong><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=J(w,b)"><img src="http://www.zhihu.com/equation?tex=J(w,b)" alt="J(w,b)"></a><strong>的最小值，从而达到加速收敛的效果</strong>。[<img src="http://www.zhihu.com/equation?tex=%5E%7B%5B1%5D%7D" alt="^{[1]}](http://www.zhihu.com/equation?tex=%5E%7B%5B1%5D%7D)"></p>
<p><em>注：上图来源于 Andrew Ng 的课程讲义</em></p>
<h2 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a><strong>Batch Normalization</strong></h2><p>在机器学习中，<strong>最常用标准化的地方</strong>莫过于神经网络的 <strong>BN 层（Batch Normalization）</strong>，因此我们简单的谈谈 BN 层的原理和作用，想要更深入的了解可以<a href="http://link.zhihu.com/?target=https://arxiv.org/abs/1502.03167">查看论文</a>。</p>
<p>我们知道数据预处理做标准化可以加速收敛，同理，在神经网络使用标准化也可以<strong>加速收敛</strong>，而且还有如下好处：</p>
<ul>
<li>具有正则化的效果（Batch Normalization reglarizes the model）</li>
<li>提高模型的泛化能力（Be advantageous to the generalization of network）</li>
<li>允许更高的学习速率从而加速收敛（Batch Normalization enables higher learning rates）</li>
</ul>
<p>其原理是<strong>利用正则化减少内部相关变量分布的偏移（Reducing Internal Covariate Shift）</strong>，从而<strong>提高了算法的鲁棒性</strong>。[<img src="http://www.zhihu.com/equation?tex=%5E%7B%5B2%5D%7D" alt="^{[2]}](http://www.zhihu.com/equation?tex=%5E%7B%5B2%5D%7D)"></p>
<p>Batch Normalization 由两部分组成，第一部分是<strong>缩放与平移（scale and shift）</strong>，第二部分是<strong>训练缩放尺度和平移的参数（train a BN Network）</strong>，算法步骤如下：</p>
<p><a target="_blank" rel="noopener" href="https://s2.ax1x.com/2019/05/12/E4KCHH.jpg"><img src="https://s2.ax1x.com/2019/05/12/E4KCHH.jpg" alt="img"></a></p>
<p>接下来训练 BN 层参数<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cgamma"><img src="http://www.zhihu.com/equation?tex=%5Cgamma" alt="\gamma"></a>和<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cbeta+"><img src="http://www.zhihu.com/equation?tex=%5Cbeta+" alt="\beta "></a>，限于篇幅的原因按下不表，有兴趣的读者可以拜读<a href="http://link.zhihu.com/?target=https://arxiv.org/abs/1502.03167">这篇论文</a>。</p>
<h2 id="0x03-正则化-Regularization"><a href="#0x03-正则化-Regularization" class="headerlink" title="0x03 正则化 Regularization"></a><strong>0x03 正则化 Regularization</strong></h2><p><strong>正则化主要用于避免过拟合的产生和减少网络误差。</strong></p>
<p>正则化一般具有如下形式：</p>
<p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=J(w,b)=+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(f(x),y)+%5Clambda+R(f)"><img src="http://www.zhihu.com/equation?tex=J(w,b)=+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(f(x),y)+%5Clambda+R(f)" alt="J(http://www.zhihu.com/equation?tex=J%28w%2Cb%29%3D+%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi%3D1%7D%5E%7Bm%7DL%28f%28x%29%2Cy%29%2B%5Clambda+R%28f%29)= \frac{1}{m} \sum_{i=1}^{m}L(f(x),y)+\lambda R(f)"></a></p>
<p>其中，第 1 项是<strong>经验风险</strong>，第 2 项是<strong>正则项</strong>，<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%CE%BB%E2%89%A50"><img src="http://www.zhihu.com/equation?tex=%CE%BB%E2%89%A50" alt="λ≥0"></a>为调整两者之间关系的系数。</p>
<p>第 1 项的经验风险较小的模型可能较复杂（有多个非零参数），这时第 2 项的模型复杂度会较大。</p>
<p>常见的有正则项有 <strong>L1 正则</strong> 和 <strong>L2 正则</strong> ，其中 <strong>L2 正则</strong> 的控制过拟合的效果比 <strong>L1 正则</strong> 的好。</p>
<p><strong>正则化的作用是选择经验风险与模型复杂度同时较小的模型</strong>。[<img src="http://www.zhihu.com/equation?tex=%5E%7B%5B3%5D%7D" alt="^{[3]}](http://www.zhihu.com/equation?tex=%5E%7B%5B3%5D%7D)"></p>
<p>常见的有正则项有 <strong>L1 正则</strong> 和 <strong>L2 正则</strong> 以及 <strong>Dropout</strong> ，其中 <strong>L2 正则</strong> 的控制过拟合的效果比 <strong>L1 正则</strong> 的好。</p>
<h2 id="范数"><a href="#范数" class="headerlink" title="范数"></a><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7Bp%7D"><img src="http://www.zhihu.com/equation?tex=L_%7Bp%7D" alt="L_{p}"></a><strong>范数</strong></h2><p>为什么叫 L1 正则，有 L1、L2 正则 那么有没有 L3、L4 之类的呢？</p>
<p>首先我们补一补课，<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7Bp%7D"><img src="http://www.zhihu.com/equation?tex=L_%7Bp%7D" alt="L_{p}"></a>正则的 L 是指<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7Bp%7D"><img src="http://www.zhihu.com/equation?tex=L_%7Bp%7D" alt="L_{p}"></a>范数，其定义为：</p>
<p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7B0%7D"><img src="http://www.zhihu.com/equation?tex=L_%7B0%7D" alt="L_{0}"></a>范数：<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cleft+%7C+w+%5Cright+%7C_%7B0%7D+=+%23(i)+with++x_%7Bi%7D+%5Cneq+0"><img src="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B0%7D+=+%5C%23(i)%5C+with+%5C+x_%7Bi%7D+%5Cneq+0" alt="\left \| w \right \|_{0} = \#(i)\ with \ x_{i} \neq 0"></a><em>（非零元素的个数）</em></p>
<p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7B1%7D"><img src="http://www.zhihu.com/equation?tex=L_%7B1%7D" alt="L_{1}"></a>范数：<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cleft+%7C+w+%5Cright+%7C_%7B1%7D+=+%5Csum_%7Bi+=+1%7D%5E%7Bd%7D%5Clvert+x_i%5Crvert"><img src="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D+=+%5Csum_%7Bi+=+1%7D%5E%7Bd%7D%5Clvert+x_i%5Crvert" alt="\left \| w \right \|_{1} = \sum_{i = 1}^{d}\lvert x_i\rvert"></a><em>（每个元素绝对值之和）</em></p>
<p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7B2%7D"><img src="http://www.zhihu.com/equation?tex=L_%7B2%7D" alt="L_{2}"></a>范数：<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cleft+%7C+w+%5Cright+%7C_%7B2%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5E2%5CBigr)%5E%7B1/2%7D"><img src="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B2%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5E2%5CBigr)%5E%7B1/2%7D" alt="\left \| w \right \|_{2} = \Bigl(\sum_{i = 1}^{d} x_i^2\Bigr)^{1/2}"></a><em>（欧氏距离）</em></p>
<p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7Bp%7D"><img src="http://www.zhihu.com/equation?tex=L_%7Bp%7D" alt="L_{p}"></a>范数：<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5Cleft+%7C+w+%5Cright+%7C_%7Bp%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5Ep%5CBigr)%5E%7B1/p%7D"><img src="http://www.zhihu.com/equation?tex=%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7Bp%7D+=+%5CBigl(%5Csum_%7Bi+=+1%7D%5E%7Bd%7D+x_i%5Ep%5CBigr)%5E%7B1/p%7D" alt="\left \| w \right \|_{p} = \Bigl(\sum_{i = 1}^{d} x_i^p\Bigr)^{1/p}"></a></p>
<p>在机器学习中，若使用了<a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=%5ClVert+w%5CrVert_p"><img src="http://www.zhihu.com/equation?tex=%5ClVert+w%5CrVert_p" alt="\lVert w\rVert_p"></a>作为正则项，我们则说该机器学习任务<strong>引入了</strong><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=L_%7Bp%7D"><img src="http://www.zhihu.com/equation?tex=L_%7Bp%7D" alt="L_{p}"></a><strong>正则项</strong>。</p>
<p><a target="_blank" rel="noopener" href="https://s2.ax1x.com/2019/05/12/E4KFUA.jpg"><img src="https://s2.ax1x.com/2019/05/12/E4KFUA.jpg" alt="img"></a></p>
<p><em>上图来自周志华老师的《机器学习》插图</em></p>
<h2 id="L1-正则-Lasso-regularizer"><a href="#L1-正则-Lasso-regularizer" class="headerlink" title="L1 正则 Lasso regularizer"></a><strong>L1 正则 Lasso regularizer</strong></h2><p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7Bm%7D%5Cleft+%7C+w+%5Cright+%7C_%7B1%7D"><img src="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7Bm%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C_%7B1%7D" alt="J(w,b)=\frac{1}{m} \sum_{i=1}^{m}L(\hat{y},y)+\frac{\lambda }{m}\left \| w \right \|_{1}"></a></p>
<ul>
<li>凸函数，不是处处可微分</li>
<li>得到的是稀疏解（最优解常出现在顶点上，且顶点上的 w 只有很少的元素是非零的）</li>
</ul>
<h2 id="L2-正则-Ridge-Regularizer-Weight-Decay"><a href="#L2-正则-Ridge-Regularizer-Weight-Decay" class="headerlink" title="L2 正则 Ridge Regularizer / Weight Decay"></a><strong>L2 正则 Ridge Regularizer / Weight Decay</strong></h2><p><a target="_blank" rel="noopener" href="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7B2m%7D%5Cleft+%7C+w+%5Cright+%7C%5E%7B2%7D_%7B2%7D"><img src="http://www.zhihu.com/equation?tex=J(w,b)=%5Cfrac%7B1%7D%7Bm%7D+%5Csum_%7Bi=1%7D%5E%7Bm%7DL(%5Chat%7By%7D,y)+%5Cfrac%7B%5Clambda+%7D%7B2m%7D%5Cleft+%5C%7C+w+%5Cright+%5C%7C%5E%7B2%7D_%7B2%7D" alt="J(w,b)=\frac{1}{m} \sum_{i=1}^{m}L(\hat{y},y)+\frac{\lambda }{2m}\left \| w \right \|^{2}_{2}"></a></p>
<ul>
<li>凸函数，处处可微分</li>
<li>易于优化</li>
</ul>
<h2 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a><strong>Dropout</strong></h2><p><strong>Dropout 主要用于神经网络，其原理是使神经网络中的某些神经元随机失活，让模型不过度依赖某一神经元，达到增强模型鲁棒性以及控制过拟合的效果。</strong></p>
<p>除此之外，Dropout 还有多模型投票等功能，若有兴趣可以拜读<a href="http://link.zhihu.com/?target=https://www.cs.toronto.edu/~hinton/absps/JMLRdropout.pdf">这篇论文</a><strong>。</strong></p>
<p><a target="_blank" rel="noopener" href="https://s2.ax1x.com/2019/05/12/E4KpuD.jpg"><img src="https://s2.ax1x.com/2019/05/12/E4KpuD.jpg" alt="img"></a></p>
<h2 id="0x04-Reference"><a href="#0x04-Reference" class="headerlink" title="0x04 Reference"></a><strong>0x04 Reference</strong></h2><p>[1] LeCun, Y., Bottou, L., Orr, G., and Muller, K. Efficient backprop. In Orr, G. and K., Muller (eds.), Neural Net-works: Tricks of the trade. Springer, 1998b.</p>
<p>[2] Sergey Ioffe, Christian Szegedy, “Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift”, arXiv preprint arXiv:1502.03167, 2015.</p>
<p>[3] 李航. 统计方法学 P13-14</p>
<p>[4] 聊聊机器学习中的损失函数 <a href="http://link.zhihu.com/?target=http://kubicode.me/2016/04/11/Machine%20Learning/Say-About-Loss-Function/">http://kubicode.me/2016/04/11/Machine%20Learning/Say-About-Loss-Function/</a></p>
<p>[5] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever and Ruslan Salakhutdinov, “Dropout: A Simple Way to Prevent Neural Networks from<br>Overfitting”,</p>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/09/17/ML%20%E5%85%A5%E9%97%A8%EF%BC%9A%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/" data-id="cktocv4va0000yutk9o8q5dd7" data-title="ML 入门：归一化、标准化和正则化" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2021/09/17/NonLocal%20Neural%20Network%E5%A4%87%E5%BF%98/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          NonLocal Neural Network备忘
        
      </div>
    </a>
  
  
    <a href="/2021/09/17/%E9%87%8F%E5%AD%90%E8%AE%A1%E7%AE%97%E7%AE%80%E8%BF%B0%20-%20intro/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">量子计算简述 - intro</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E8%AE%B0/" rel="tag">日记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%97%A5%E8%AE%B0/" style="font-size: 10px;">日记</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/09/17/SuperResolution%20by%20VDSR,%20PerceptualSR,%20SubpixelConvSR%EF%BC%8CESRGAN/">SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN</a>
          </li>
        
          <li>
            <a href="/2021/09/17/SuperResolution%20by%20FSRCNN/">SuperResolution by FSRCNN</a>
          </li>
        
          <li>
            <a href="/2021/09/17/tfRecord%20%E6%90%AC%E8%BF%90/">tfRecord 搬运</a>
          </li>
        
          <li>
            <a href="/2021/09/17/NonLocal%20Neural%20Network%E5%A4%87%E5%BF%98/">NonLocal Neural Network备忘</a>
          </li>
        
          <li>
            <a href="/2021/09/17/ML%20%E5%85%A5%E9%97%A8%EF%BC%9A%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/">ML 入门：归一化、标准化和正则化</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Yubinn Zeng<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>