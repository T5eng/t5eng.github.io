<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  
  <title>SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN | Yubinnzeng&#39;s Blog</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
  <meta name="description" content="以下内容基于本人阅读理解 VDSR: arXiv:1511.04587v2 [cs.CV] 11 Nov 2016 Perceptual Loss SR: arXiv:1609.05158v2 [cs.CV] 23 Sep 2016 Subpixel SR: arXiv:1603.08155v1 [cs.CV] 27 Mar 2016 SRGAN: arXiv:1609.04802v5 [cs.C">
<meta property="og:type" content="article">
<meta property="og:title" content="SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN">
<meta property="og:url" content="http://example.com/2021/09/17/SuperResolution%20by%20VDSR,%20PerceptualSR,%20SubpixelConvSR%EF%BC%8CESRGAN/index.html">
<meta property="og:site_name" content="Yubinnzeng&#39;s Blog">
<meta property="og:description" content="以下内容基于本人阅读理解 VDSR: arXiv:1511.04587v2 [cs.CV] 11 Nov 2016 Perceptual Loss SR: arXiv:1609.05158v2 [cs.CV] 23 Sep 2016 Subpixel SR: arXiv:1603.08155v1 [cs.CV] 27 Mar 2016 SRGAN: arXiv:1609.04802v5 [cs.C">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcgy1g119z3tje0j31c40bqjzt.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcgy1g11atcmtsrj31bu0g4dp9.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g11ctpmiabj30ii02qjrl.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g11dojzc0jj30oi03mq3b.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g11e7ssmdfj30fu01s0su.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcgy1g169htpdtdj311k0csadz.jpg">
<meta property="og:image" content="https://ws4.sinaimg.cn/large/006tKfTcgy1g16w2bk0r6j30wc02uwex.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcgy1g174h2fsrcj317y0b2jwi.jpg">
<meta property="og:image" content="https://ws2.sinaimg.cn/large/006tKfTcgy1g177vdw0o4j319u080go1.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c13mpx2j30se0ionfc.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c61avdqj30om04a750.jpg">
<meta property="og:image" content="https://ws3.sinaimg.cn/large/006tKfTcgy1g188yi3ju6j30j605mmxs.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g18a3yud9wj30h203udg4.jpg">
<meta property="og:image" content="https://ws1.sinaimg.cn/large/006tKfTcgy1g18dy21lhbj30rq0ca42t.jpg">
<meta property="article:published_time" content="2021-09-17T12:26:48.045Z">
<meta property="article:modified_time" content="2021-09-17T12:35:47.830Z">
<meta property="article:author" content="Yubinn Zeng">
<meta property="article:tag" content="深度学习">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://ws4.sinaimg.cn/large/006tKfTcgy1g119z3tje0j31c40bqjzt.jpg">
  
    <link rel="alternate" href="/atom.xml" title="Yubinnzeng's Blog" type="application/atom+xml">
  
  
    <link rel="shortcut icon" href="/favicon.png">
  
  
    
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/typeface-source-code-pro@0.0.71/index.min.css">

  
  
<link rel="stylesheet" href="/css/style.css">

  
    
<link rel="stylesheet" href="/fancybox/jquery.fancybox.min.css">

  
<meta name="generator" content="Hexo 5.4.0"></head>

<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Yubinnzeng&#39;s Blog</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://example.com"></form>
      </div>
    </div>
  </div>
</header>

      <div class="outer">
        <section id="main"><article id="post-SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN" class="h-entry article article-type-post" itemprop="blogPost" itemscope itemtype="https://schema.org/BlogPosting">
  <div class="article-meta">
    <a href="/2021/09/17/SuperResolution%20by%20VDSR,%20PerceptualSR,%20SubpixelConvSR%EF%BC%8CESRGAN/" class="article-date">
  <time class="dt-published" datetime="2021-09-17T12:26:48.045Z" itemprop="datePublished">2021-09-17</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="p-name article-title" itemprop="headline name">
      SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN
    </h1>
  

      </header>
    
    <div class="e-content article-entry" itemprop="articleBody">
      
        <p>以下内容基于本人阅读理解</p>
<p>VDSR: arXiv:1511.04587v2 [cs.CV] 11 Nov 2016</p>
<p>Perceptual Loss SR: arXiv:1609.05158v2 [cs.CV] 23 Sep 2016</p>
<p>Subpixel SR: arXiv:1603.08155v1 [cs.CV] 27 Mar 2016</p>
<p>SRGAN: arXiv:1609.04802v5 [cs.CV] 25 May 2017</p>
<p>ESRGAN: arXiv:1809.00219v2 [cs.CV] 17 Sep 2018</p>
<p>4个改进DeepLearning方法实现超分辨率的代表作。</p>
<hr>
<h2 id="Abstract"><a href="#Abstract" class="headerlink" title="Abstract"></a>Abstract</h2><p>srcnn证明了deep cnn end2end可以实现优于传统方法的超分辨率任务。</p>
<p>本文介绍的4篇论文分别从网络结构、损失函数、upsample层、GAN四种不同角度优化原始的超分辨率网络。</p>
<hr>
<h2 id="VeryDeepSuperResolution"><a href="#VeryDeepSuperResolution" class="headerlink" title="VeryDeepSuperResolution"></a>VeryDeepSuperResolution</h2><p>原始的srcnn只用了3层，fsrcnn用了15层，已经非常难训练，拟合很慢。VDSR加上了残差连接、gradient clipping，实现了20层的网络和极高的learning rate，以及有更大的感知域（41x41）和在同一个网络实现不同upscale factor。</p>
<h3 id="Residual-Learning"><a href="#Residual-Learning" class="headerlink" title="Residual Learning"></a>Residual Learning</h3><p><a target="_blank" rel="noopener" href="https://ws4.sinaimg.cn/large/006tKfTcgy1g119z3tje0j31c40bqjzt.jpg"><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g119z3tje0j31c40bqjzt.jpg" alt="image-20190313120326694"></a></p>
<p>interpolation到hr分辨率，经过20层Conv后输出r，加上残差连接x，输出SR结果，因此r = y - x。Loss函数为x+r与y的Euclidean distance。</p>
<h3 id="Gradient-Clipping"><a href="#Gradient-Clipping" class="headerlink" title="Gradient Clipping"></a>Gradient Clipping</h3><p>一般的clipping用[−θ, θ]，缺点是即使使用更大的lr，梯度也依然限制在[−θ, θ]内。因此提出可以动态clipping的方法：[-θ/lr , θ/lr]，这样就可以根据lr的策略动态控制Gradient Clipping的范围。</p>
<h3 id="Multi—Scale"><a href="#Multi—Scale" class="headerlink" title="Multi—Scale"></a>Multi—Scale</h3><p>因为网络只负责reconstruction，上采样是用传统方法完成的。因此一个网络就可以实现多种不同的scale。</p>
<h3 id="Result"><a href="#Result" class="headerlink" title="Result"></a>Result</h3><p><a target="_blank" rel="noopener" href="https://ws3.sinaimg.cn/large/006tKfTcgy1g11atcmtsrj31bu0g4dp9.jpg"><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g11atcmtsrj31bu0g4dp9.jpg" alt="image-20190313123237150"></a></p>
<p>20层的conv的runtime居然比3层conv的srcnn还快？？？</p>
<hr>
<h2 id="PerceptualSR"><a href="#PerceptualSR" class="headerlink" title="PerceptualSR"></a>PerceptualSR</h2><p>PSNR（PeakSignalNoiseRatio）是通用的对比图片相似度的标准，与MSE成正比，因此很多CV任务的loss函数都采用了MSE，即输出和Ground Truth的每个像素的MSE。而Perceptual Loss采用了VGG中间某几层的activation output作为loss。在StyleTransfer和SuperResolution都得到了不错的结果。</p>
<h3 id="Feature-Reconstruction-Loss"><a href="#Feature-Reconstruction-Loss" class="headerlink" title="Feature Reconstruction Loss"></a>Feature Reconstruction Loss</h3><p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g11ctpmiabj30ii02qjrl.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g11ctpmiabj30ii02qjrl.jpg" alt="image-20190313134211674"></a></p>
<p>其中：φ j (y)是VGG的第j层的activation output。</p>
<p>相当于借用VGG做feature extraction，feat Loss是图片在higher level feature map的<strong>EuclideanNorm</strong>，而不是像素级别的距离，保存了图片的semantic和空间结构，但是没有考虑到色彩、纹理和形状。</p>
<h3 id="Style-Reconstruction-Loss"><a href="#Style-Reconstruction-Loss" class="headerlink" title="Style Reconstruction Loss"></a>Style Reconstruction Loss</h3><p>先定义一个gram matrix：</p>
<p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g11dojzc0jj30oi03mq3b.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g11dojzc0jj30oi03mq3b.jpg" alt="image-20190313141150026"></a></p>
<p>返回一个Cj x Cj的矩阵，计算φ j (y)的c通道和c‘通道对应的elementwise product的和。相当于计算channel wise的covariance。</p>
<p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g11e7ssmdfj30fu01s0su.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g11e7ssmdfj30fu01s0su.jpg" alt="image-20190313143020157"></a></p>
<p>style Loss是2个图片的GramMatrix差值的<strong>FrobeniusNorm</strong>。</p>
<h3 id="More-Specific"><a href="#More-Specific" class="headerlink" title="More Specific"></a>More Specific</h3><p><a target="_blank" rel="noopener" href="https://ws4.sinaimg.cn/large/006tKfTcgy1g169htpdtdj311k0csadz.jpg"><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g169htpdtdj311k0csadz.jpg" alt="image-20190317193438543"></a></p>
<p>论文主要是利用了VGG16作为LossNetwork计算Loss，ImageTransformNet可以套用各种不同的网络。</p>
<p>做StyleTransfer时，需要同时使用ys，y_hat，yc作为LossNet的输入。</p>
<p><a target="_blank" rel="noopener" href="https://ws4.sinaimg.cn/large/006tKfTcgy1g16w2bk0r6j30wc02uwex.jpg"><img src="https://ws4.sinaimg.cn/large/006tKfTcgy1g16w2bk0r6j30wc02uwex.jpg" alt="image-20190317234812204"></a></p>
<p>做SR任务时，仅将SR作为y_hat，HR作为yc，选用<strong>relu2_2层</strong>计算FeatureLoss。</p>
<h3 id="Experiment"><a href="#Experiment" class="headerlink" title="Experiment"></a>Experiment</h3><p>论文展示了x4和x8，估计在x2和x3上表现不如其他方法。</p>
<p>图片downsample前先进行高斯模糊（σ = 1.0）</p>
<p>卷积层后面接spacial batch normalization</p>
<p>选了VGG16第2层的输出作为φ(y)计算feat Loss，基于srcnn修改</p>
<p>batch size = 4，计算200k步</p>
<p>Adam，lr=1e-3</p>
<p>test时先进行histogram matching处理，再计算psnr（偏高？）</p>
<h3 id="Result-1"><a href="#Result-1" class="headerlink" title="Result"></a>Result</h3><p>细节和边缘表现优秀</p>
<p>放大会看到部分色块超出原有边界</p>
<hr>
<h2 id="SubpixelConvSR-ESPCN"><a href="#SubpixelConvSR-ESPCN" class="headerlink" title="SubpixelConvSR (ESPCN)"></a>SubpixelConvSR (ESPCN)</h2><p>此前SR任务的upsample操作都是通过fractional convolution进行的，此论文提出了新的upsample方法：子像素重排列。</p>
<h3 id="Network-Architecture"><a href="#Network-Architecture" class="headerlink" title="Network Architecture"></a>Network Architecture</h3><p><a target="_blank" rel="noopener" href="https://ws2.sinaimg.cn/large/006tKfTcgy1g174h2fsrcj317y0b2jwi.jpg"><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g174h2fsrcj317y0b2jwi.jpg" alt="image-20190318132622948"></a></p>
<p>全卷积，逐层增加通道数</p>
<p>输入通道为c，upscale factor为r的话，最终通道数为c <em>r^2，即输出为</em><em>[h, w, c</em> r^2]**</p>
<p>Sub-pixel cone layer通过像素重新排列（不需要计算），将feature map转换为**[h *r, w* r, c]**，实现上采样，论文中命名为periodic shuffling operator (PS)。由于这个过程不需要计算，所以速度非常快。</p>
<p>参考SRCNN的3层网络结构，第一层conv(5x5, c, 64) 第二层conv(3x3, 64, 32) 第三层conv(3x3, 32, <strong>c*r^2</strong>) 第四层subpixel(r)</p>
<h3 id="Experiment-1"><a href="#Experiment-1" class="headerlink" title="Experiment"></a>Experiment</h3><p>图片分辨率为(17r x 17r)，downsample前先进行高斯模糊</p>
<p>激活函数选tanh</p>
<p>训练100轮，lr=0.01，每1轮lr=lr*0.1</p>
<p>用K2训练，91images上训练了3hr，另外一个模型在ImageNet上训练了7天。</p>
<p>PSNR用matlab算（比较准）</p>
<h3 id="Result-2"><a href="#Result-2" class="headerlink" title="Result"></a>Result</h3><p>如果激活函数跟SRCNN一样用relu，对比数据集分别用91images和ImageNet，结果显示ESPCN在更大的数据集上会有进一步的表现(+0.33)，而SRCNN不会(+0.07)</p>
<p><a target="_blank" rel="noopener" href="https://ws2.sinaimg.cn/large/006tKfTcgy1g177vdw0o4j319u080go1.jpg"><img src="https://ws2.sinaimg.cn/large/006tKfTcgy1g177vdw0o4j319u080go1.jpg" alt="image-20190318151055556"></a></p>
<p>1080p的图片在GPU上超分辨率可以做到<strong>0.038s</strong>每帧（算是实时？），而SRCNN需要<strong>0.435s</strong>。</p>
<h2 id="Enhanced-SuperResolution-GAN-ESRGAN"><a href="#Enhanced-SuperResolution-GAN-ESRGAN" class="headerlink" title="Enhanced SuperResolution GAN(ESRGAN)"></a>Enhanced SuperResolution GAN(ESRGAN)</h2><p>SRGAN的升级版，用生成对抗的方法实现超分辨率任务。与PerceptualLoss类似，只是这次的LossNetwork是可训练的，结果同样是在<strong>PSNR比较低</strong>的情况下得到<strong>感官效果更优秀</strong>的超分辨率图片。并且在超大upscale factor上的表现比PSNR方法的表现更好。</p>
<h3 id="前作SRGAN"><a href="#前作SRGAN" class="headerlink" title="前作SRGAN"></a>前作SRGAN</h3><p>提出一个<strong>mean-opinion score</strong>（MOS），找26个人评分（1~5）然后求均值。</p>
<p>GAN网络：Generator采用ResNet网络，Discriminator采用VGG。</p>
<p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c13mpx2j30se0ionfc.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c13mpx2j30se0ionfc.jpg" alt="image-20190318173207720"></a></p>
<p>从manifold的角度分析：将patch映射到2维空间，可以发现GAN的结果与真实结果分布比较接近，而以MSE为objective function的结果偏离了真实分布，所以画面偏smooth。</p>
<h4 id="SRGAN-Loss"><a href="#SRGAN-Loss" class="headerlink" title="SRGAN Loss"></a>SRGAN Loss</h4><p>Loss函数采用，典型的GAN的min-max函数（基于binomial的loss，然后训练Discriminator最大化误差，训练Generator最小化误差）</p>
<p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c61avdqj30om04a750.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g17c61avdqj30om04a750.jpg" alt="image-20190318175244115"></a></p>
<p><a target="_blank" rel="noopener" href="https://ws3.sinaimg.cn/large/006tKfTcgy1g188yi3ju6j30j605mmxs.jpg"><img src="https://ws3.sinaimg.cn/large/006tKfTcgy1g188yi3ju6j30j605mmxs.jpg" alt="image-20190319124717727"></a></p>
<p>其中：content loss可以选择通常使用的MSE或者基于VGG的perceptual loss，adversarial loss就是Discriminator的输出</p>
<p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g18a3yud9wj30h203udg4.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g18a3yud9wj30h203udg4.jpg" alt="image-20190319132712222"></a></p>
<p>论文中说，adversarial loss是可以让生成图片更接近实际图片的分布。</p>
<p>没有采用cross entropy的log[1-D( G(input) )]而是采用了 -log( D( G(input) )</p>
<h3 id="Experiment-2"><a href="#Experiment-2" class="headerlink" title="Experiment"></a>Experiment</h3><p>在ImageNet中随机选了350k张图片剪裁成96x96的patch。图片先进行高斯模糊，再downsample。LR preprocess为[0,1]，HR为[-1,1]。MSE也是在[-1,1]上计算，VGGLoss需要乘以1/12.75</p>
<p>batch size = 16</p>
<p>optimizer: Adam, beta1=0.9</p>
<p>lr = 1e-4 1e6 iterations</p>
<p>选用预训练SRResNet</p>
<p>测试时去掉边缘4个像素（upsample时由于边缘像素的效果比较差）</p>
<h4 id="Result-3"><a href="#Result-3" class="headerlink" title="Result"></a>Result</h4><p><a target="_blank" rel="noopener" href="https://ws1.sinaimg.cn/large/006tKfTcgy1g18dy21lhbj30rq0ca42t.jpg"><img src="https://ws1.sinaimg.cn/large/006tKfTcgy1g18dy21lhbj30rq0ca42t.jpg" alt="image-20190319153954384"></a></p>
<p>用MOS计算，SRGAN是最高分。可是PSNR只有29.4dB(set5)和26.02dB(set14)。</p>
<p>与之前的研究结论不一样，论文认为更深的残差网络可以进一步提高效果，但训练时间更长</p>
<h3 id="ESRGAN本体"><a href="#ESRGAN本体" class="headerlink" title="ESRGAN本体"></a>ESRGAN本体</h3><p>在SRGAN的基础上：</p>
<ol>
<li>提出Res-in-Res Dense Block，更容易训练。移除Batch Norm，改为Res Scaling（将res的output乘以一个scale之后再加input）</li>
<li>改用RelativisticGAN。</li>
<li>VGGLoss改用activate前的feature map。</li>
</ol>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://example.com/2021/09/17/SuperResolution%20by%20VDSR,%20PerceptualSR,%20SubpixelConvSR%EF%BC%8CESRGAN/" data-id="cktocv4vk0008yutkcm72etz1" data-title="SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN" class="article-share-link">Share</a>
      
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
  
    <a href="/2021/09/17/SuperResolution%20by%20FSRCNN/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">SuperResolution by FSRCNN</div>
    </a>
  
</nav>

  
</article>


</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tags</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%97%A5%E8%AE%B0/" rel="tag">日记</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag">深度学习</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Tag Cloud</h3>
    <div class="widget tagcloud">
      <a href="/tags/%E6%97%A5%E8%AE%B0/" style="font-size: 10px;">日记</a> <a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" style="font-size: 20px;">深度学习</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2021/09/">September 2021</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/01/">January 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/09/">September 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/07/">July 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/06/">June 2018</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/01/">January 2018</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2021/09/17/SuperResolution%20by%20VDSR,%20PerceptualSR,%20SubpixelConvSR%EF%BC%8CESRGAN/">SuperResolution by VDSR, PerceptualSR, SubpixelConvSR，ESRGAN</a>
          </li>
        
          <li>
            <a href="/2021/09/17/SuperResolution%20by%20FSRCNN/">SuperResolution by FSRCNN</a>
          </li>
        
          <li>
            <a href="/2021/09/17/tfRecord%20%E6%90%AC%E8%BF%90/">tfRecord 搬运</a>
          </li>
        
          <li>
            <a href="/2021/09/17/NonLocal%20Neural%20Network%E5%A4%87%E5%BF%98/">NonLocal Neural Network备忘</a>
          </li>
        
          <li>
            <a href="/2021/09/17/ML%20%E5%85%A5%E9%97%A8%EF%BC%9A%E5%BD%92%E4%B8%80%E5%8C%96%E3%80%81%E6%A0%87%E5%87%86%E5%8C%96%E5%92%8C%E6%AD%A3%E5%88%99%E5%8C%96/">ML 入门：归一化、标准化和正则化</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      
      &copy; 2021 Yubinn Zeng<br>
      Powered by <a href="https://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>

    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    


<script src="/js/jquery-3.4.1.min.js"></script>



  
<script src="/fancybox/jquery.fancybox.min.js"></script>




<script src="/js/script.js"></script>





  </div>
</body>
</html>